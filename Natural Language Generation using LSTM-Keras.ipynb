{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import unicodedata\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the movie conversation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have stored the converstion in pickle file so you can escape these below steps and directly get the conversations from the pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='data/'\n",
    "print (os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc=[]\n",
    "train_dec=[]\n",
    "f1=open('data/train.enc','rb')\n",
    "f2=open('data/train.dec','rb')\n",
    "for i in f1.readlines():\n",
    "    try:\n",
    "        temp1=i.decode(\"utf-8\",errors='replace')\n",
    "        print (temp1[0])\n",
    "    except Exception as e:\n",
    "        print ('***********')\n",
    "        print (i)\n",
    "        break\n",
    "    train_enc.append(temp1)\n",
    "    \n",
    "for j in f2.readlines():\n",
    "    try:\n",
    "        temp2=j.decode(\"utf-8\",errors='replace')\n",
    "        print (temp2[0])\n",
    "    except Exception as e:\n",
    "        print ('***********')\n",
    "        print (i)\n",
    "        break\n",
    "    train_dec.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc=[]\n",
    "test_dec=[]\n",
    "f1=open('data/test.enc','rb')\n",
    "f2=open('data/test.dec','rb')\n",
    "for i in f1.readlines():\n",
    "    try:\n",
    "        temp1=i.decode(\"utf-8\",errors='replace')\n",
    "        print (temp1[0])\n",
    "    except Exception as e:\n",
    "        print ('***********')\n",
    "        print (i)\n",
    "        break\n",
    "    test_enc.append(temp1)\n",
    "    \n",
    "for j in f2.readlines():\n",
    "    try:\n",
    "        temp2=j.decode(\"utf-8\",errors='replace')\n",
    "        print (temp2[0])\n",
    "    except Exception as e:\n",
    "        print ('***********')\n",
    "        print (i)\n",
    "        break\n",
    "    test_dec.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_enc[0:2],train_dec[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list=[]\n",
    "for i,j in zip(train_enc,train_dec):\n",
    "    train_list.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(train_list,columns=['person1','person2'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For doing preprocessing of text you can alter below code to work on both person1 and person2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import re\n",
    "import numpy as np\n",
    "# #stop=['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}','/','-']\n",
    "# def number_removal(row):\n",
    "#     data1 = row['Summary']\n",
    "#     #print data1\n",
    "#     #tokens = nltk.wordpunct_tokenize(data)\n",
    "#     #print (type(data1))\n",
    "#     if type(data1) not in [int,float]:\n",
    "#         print type(data1)\n",
    "#         line = re.sub(r\"[^A-Za-z\\s]\", \" \", data1.strip())\n",
    "#         tokens = line.split()\n",
    "#     else:\n",
    "#         tokens=[]\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# frequency_words_wo_stop = {}\n",
    "# def generate_word_frequency(row):\n",
    "#     data1 = row['Summary']\n",
    "#     tokens = nltk.wordpunct_tokenize(data1)\n",
    "#     token_list = []\n",
    "#     for token in tokens:\n",
    "# #         if token.lower() not in stop:\n",
    "# #             token_list.append(token.lower())\n",
    "#         if token.lower() in frequency_words_wo_stop:\n",
    "#             count = frequency_words_wo_stop[token.lower()]\n",
    "#             count = count + 1\n",
    "#             frequency_words_wo_stop[token.lower()] = count\n",
    "#         else:\n",
    "#             frequency_words_wo_stop[token.lower()] = 1\n",
    "    \n",
    "#     return ','.join(token_list)\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# def receieve(query):\n",
    "#     #data=pd.DataFrame([query])\n",
    "#     #data.columns=['Summary']\n",
    "#     data1=query\n",
    "#     #print (data1.head())\n",
    "#     data1['Summary'] = data1.apply(number_removal,axis=1)\n",
    "    \n",
    "#     data1['tokens'] = data1.apply(generate_word_frequency,axis=1)\n",
    "#     print (data1.head())\n",
    "#     big=[]\n",
    "#     for i in data1['tokens']:\n",
    "#         st=''\n",
    "#         ls=[]\n",
    "#         for j in i.split(','):\n",
    "#             #print j\n",
    "#             print (wordnet_lemmatizer.lemmatize(j))\n",
    "#             ls.append(wordnet_lemmatizer.lemmatize(j))\n",
    "#         #print ls\n",
    "#         big.append(' '.join(ls))\n",
    "#     data1['Summary_lem']=big\n",
    "#     return data1['Summary_lem']\n",
    "# data['Summary']=receieve(data[['Summary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "# def fun(dat):\n",
    "#     big=[]\n",
    "#     for i in dat['Question']:\n",
    "#         st=''\n",
    "#         ls=[]\n",
    "#         for j in i.split():\n",
    "#             #print j\n",
    "#             ls.append(wordnet_lemmatizer.lemmatize(re.sub(r\"[^A-Za-z\\s]\", \" \", j.strip().lower())))\n",
    "#         #print ls\n",
    "#         big.append(' '.join(ls))\n",
    "#     return big\n",
    "\n",
    "\n",
    "# data['Question']=fun(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=data['person1'].values\n",
    "y=data['person2'].values\n",
    "\n",
    "# skf = StratifiedKFold(y, n_folds=4)\n",
    "# for train_index, test_index in skf:\n",
    "#     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert words into vectors using Google's word vectors\n",
    "### Download from https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('google_word_vectors/word2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_x=[]\n",
    "tok_y=[]\n",
    "for i in range(len(X)):\n",
    "    tok_x.append(nltk.word_tokenize(X[i].lower()))\n",
    "    tok_y.append(nltk.word_tokenize(y[i].lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentend=np.ones((300,),dtype=np.float32) \n",
    "\n",
    "vec_x=[]\n",
    "for sent in tok_x:\n",
    "    sentvec = [model[w] for w in sent if w in model.vocab]\n",
    "    vec_x.append(sentvec)\n",
    "    \n",
    "vec_y=[]\n",
    "for sent in tok_y:\n",
    "    sentvec = [model[w] for w in sent if w in model.vocab]\n",
    "    vec_y.append(sentvec)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tok_sent in vec_x:\n",
    "    tok_sent[14:]=[]\n",
    "    tok_sent.append(sentend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tok_sent in vec_x:\n",
    "    if len(tok_sent)<15:\n",
    "        for i in range(15-len(tok_sent)):\n",
    "            tok_sent.append(sentend) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(max(tok_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tok_sent in vec_y:\n",
    "    tok_sent[14:]=[]\n",
    "    tok_sent.append(sentend)\n",
    "    \n",
    "\n",
    "for tok_sent in vec_y:\n",
    "    if len(tok_sent)<15:\n",
    "        for i in range(15-len(tok_sent)):\n",
    "            tok_sent.append(sentend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('conversation.pickle','wb') as f:\n",
    "    pickle.dump([vec_x,vec_y],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM,SimpleRNN\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('conversation.pickle','rb')\n",
    "vec_x,vec_y=pickle.load(f)    \n",
    "    \n",
    "vec_x=np.array(vec_x,dtype=np.float64)\n",
    "vec_y=np.array(vec_y,dtype=np.float64)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_test, y_train,y_test = train_test_split(vec_x, vec_y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init='glorot_normal', inner_init='glorot_normal', activation='sigmoid'))\n",
    "model.compile(loss='cosine_proximity', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('lstm_models/LSTM500.h5')\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('lstm_models/LSTM1000.h5')\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('lstm_models/LSTM1500.h5')\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('lstm_models/LSTM2000.h5')\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('lstm_models/LSTM2500.h5')\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('lstm_models/LSTM3000.h5')\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('lstm_models/LSTM3500.h5')\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('lstm_models/LSTM4000.h5')\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('lstm_models/LSTM4500.h5')\n",
    "model.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\n",
    "model.save('lstm_models/LSTM5000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(x_test) \n",
    "mod = gensim.models.Word2Vec.load('word2vec.bin')   \n",
    "[mod.most_similar([predictions[10][i]])[0] for i in range(15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x='what this all about'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentend=np.ones((300,),dtype=np.float32) \n",
    "\n",
    "sent=nltk.word_tokenize(x.lower())\n",
    "sentvec = [mod[w] for w in sent if w in mod.vocab]\n",
    "\n",
    "sentvec[14:]=[]\n",
    "sentvec.append(sentend)\n",
    "if len(sentvec)<15:\n",
    "    for i in range(15-len(sentvec)):\n",
    "        sentvec.append(sentend) \n",
    "sentvec=np.array([sentvec])\n",
    "\n",
    "predictions = model.predict(sentvec)\n",
    "#print (predictions)\n",
    "outputlist=[mod.most_similar([predictions[0][i]])[0][0] for i in range(15)]\n",
    "output=' '.join(outputlist)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions[0][0],predictions[0][0].shape,predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict_classes(sentvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod.most_similar([predictions[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feel free to improve it and provide feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
